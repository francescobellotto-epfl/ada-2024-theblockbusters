{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge People Datasets\n",
    "\n",
    "In this notebook, we merge two distinct people datasets to create a comprehensive and enriched dataset:\n",
    "\n",
    "1. **MovieSummaries**: Provides anagraphic information.\n",
    "2. **Wikidata Dataset**: Scraped data providing supplementary anagraphic information about people.\n",
    "\n",
    "### Merging Process\n",
    "- We first outer merge **MovieSummaries** with **Wikidata**.\n",
    "- Then, we create an univocal key to identify people in our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import unicodedata\n",
    "from datetime import datetime\n",
    "\n",
    "from auxiliary_functions_for_merging import *\n",
    "\n",
    "DATA_PATH = \"./../../Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and clean MovieSummaries dataset\n",
    "\n",
    "people_original_cols = [\"wikipedia_id_movie\", \"freebase_id_movie\", \"release_date\", \"character_name\", \"date_of_birth\", \"gender\", \"height\",\n",
    "                           \"freebase_id_etnicity\", \"name_actor\", \"age_actor\", \"freebase_id_character_actor\", \"freebase_id_character\",\n",
    "                           \"freebase_id_actor\"]\n",
    "people_original = pd.read_csv(DATA_PATH + 'character.metadata.tsv', sep='\\t', header=None, names=people_original_cols).dropna(subset=['freebase_id_actor']).drop_duplicates(subset=[\"freebase_id_actor\"])\n",
    "cols = [\"freebase_id_actor\", \"name_actor\", \"gender\", \"date_of_birth\", \"height\", \"freebase_id_etnicity\"]\n",
    "people_original = people_original[cols]\n",
    "\n",
    "people_original[\"year_of_birth\"] = people_original[\"date_of_birth\"].apply(extract_year)\n",
    "people_original[\"date_of_birth\"] = people_original[\"date_of_birth\"].apply(lambda x: x if is_valid_date(x) else pd.NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and clean Wikidata dataset\n",
    "\n",
    "wikidata_people = pd.read_csv(DATA_PATH + \"wikidata_people.csv\").drop_duplicates(subset=[\"wikidataID\"])\n",
    "\n",
    "wikidata_people[\"birthDate\"] = wikidata_people[\"birthDate\"].apply(select_date)\n",
    "wikidata_people[\"birthYear\"] = wikidata_people[\"birthDate\"].apply(extract_year)\n",
    "wikidata_people[\"birthDate\"] = wikidata_people[\"birthDate\"].apply(lambda x: x if is_valid_date(x) else pd.NA)\n",
    "wikidata_people[\"deathDate\"] = wikidata_people[\"deathDate\"].apply(select_date)\n",
    "wikidata_people[\"deathYear\"] = wikidata_people[\"deathDate\"].apply(extract_year)\n",
    "wikidata_people[\"deathDate\"] = wikidata_people[\"deathDate\"].apply(lambda x: x if is_valid_date(x) else pd.NA)\n",
    "wikidata_people[\"gender\"] = wikidata_people[\"gender\"].apply(select_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer merge MovieSummaries and Wikidata datasets by freebaseID \n",
    "\n",
    "people_complete = pd.merge(wikidata_people, people_original, left_on=\"freebaseID\", right_on=\"freebase_id_actor\", how=\"outer\", suffixes=('','_orig'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine columns that provide the same information\n",
    "\n",
    "people_complete[\"freebaseID\"] = people_complete.apply(lambda row: row[\"freebase_id_actor\"] if pd.isna(row[\"freebaseID\"]) else row[\"freebaseID\"], axis=1)\n",
    "people_complete[\"nameSurname\"] = people_complete.apply(lambda row: row[\"name_actor\"] if pd.isna(row[\"nameSurname\"]) else row[\"nameSurname\"], axis=1)\n",
    "people_complete[\"gender\"] = people_complete.apply(lambda row: row[\"gender_orig\"] if pd.isna(row[\"gender\"]) else row[\"gender\"], axis=1)\n",
    "people_complete[\"birthDate\"] = people_complete.apply(lambda row: row[\"date_of_birth\"] if pd.isna(row[\"birthDate\"]) else row[\"birthDate\"], axis=1)\n",
    "people_complete[\"birthYear\"] = people_complete.apply(lambda row: row[\"year_of_birth\"] if pd.isna(row[\"birthYear\"]) else row[\"birthYear\"], axis=1)\n",
    "\n",
    "people_complete = people_complete.drop(columns=[\"freebase_id_actor\", \"name_actor\", \"gender_orig\", \"date_of_birth\", \"year_of_birth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define univocal key to identify people. Export the resulting dataset.\n",
    "\n",
    "renamed_cols = {\n",
    "\"imdbID\": \"imdb_id_actor\",\n",
    "\"wikidataID\": \"wikidata_id_actor\",\n",
    "\"freebaseID\": \"freebase_id_actor\",\n",
    "\"wikipediaLink\": \"wikipediaLink_actor\",\n",
    "\"nameSurname\": \"nameSurname_actor\",\n",
    "\"givenName\": \"givenName_actor\",\n",
    "\"familyName\": \"familyName_actor\",\n",
    "\"birthDate\": \"date_of_birth\",\n",
    "\"gender\": \"gender\",\n",
    "\"citizenship\": \"citizenship\",\n",
    "\"placeOfBirth\": \"place_of_birth\",\n",
    "\"nativeLanguage\": \"language\",\n",
    "\"deathDate\": \"date_of_death\",\n",
    "\"birthYear\": \"year_of_birth\",\n",
    "\"deathYear\": \"year_of_death\",\n",
    "\"height\": \"height\",\n",
    "\"freebase_id_etnicity\": \"freebase_id_etnicity\",\n",
    "}\n",
    "\n",
    "people_complete = people_complete.rename(columns=renamed_cols)\n",
    "people_complete[\"univocal_id_actor\"] = people_complete.apply(lambda row: row[\"imdb_id_actor\"] if pd.isna(row[\"freebase_id_actor\"]) or len(row[\"freebase_id_actor\"]) > 20 else row[\"freebase_id_actor\"], axis=1)\n",
    "people_complete = people_complete.drop_duplicates(subset=[\"univocal_id_actor\"])\n",
    "people_complete = people_complete[(~people_complete.duplicated(subset=[\"imdb_id_actor\"])) | (people_complete['imdb_id_actor'].isna())]\n",
    "\n",
    "cols = ['univocal_id_actor', 'freebase_id_actor', 'wikidata_id_actor', 'imdb_id_actor', 'wikipediaLink_actor', 'nameSurname_actor', 'givenName_actor',\n",
    "        'familyName_actor', 'gender', 'date_of_birth', 'year_of_birth', 'date_of_death', 'year_of_death', 'place_of_birth', 'citizenship',\n",
    "        'language', 'height', 'freebase_id_etnicity']\n",
    "people_complete = people_complete[cols]\n",
    "\n",
    "people_complete.to_csv(DATA_PATH + \"people_complete.tsv\", sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ADA_env)",
   "language": "python",
   "name": "ada_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
